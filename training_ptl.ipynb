{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_VXF-lgY6xK"
   },
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9uLzx6lY-pM"
   },
   "source": [
    "## Mount Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16921,
     "status": "ok",
     "timestamp": 1751285511354,
     "user": {
      "displayName": "Irfan Ismail",
      "userId": "15670942255868779694"
     },
     "user_tz": -420
    },
    "id": "Hgrj9daPS2H4",
    "outputId": "c8343d25-87bf-4e22-f5de-5ddb8357d254"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdIGpZKM9fRg"
   },
   "source": [
    "## Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1751272707727,
     "user": {
      "displayName": "Irfan Ismail",
      "userId": "15670942255868779694"
     },
     "user_tz": -420
    },
    "id": "Z4hM5t_T9hBO",
    "outputId": "996d31c8-b985-4348-bc94-af802c2320d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Binus/MTI/DL/PTL_component\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/MyDrive/Binus/MTI/DL/PTL_component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5570,
     "status": "ok",
     "timestamp": 1751272674022,
     "user": {
      "displayName": "Irfan Ismail",
      "userId": "15670942255868779694"
     },
     "user_tz": -420
    },
    "id": "9DiRrl_g9kxT",
    "outputId": "1546dce5-bad3-4324-a4e2-852c3182b968"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'yolov7_PTL'...\n",
      "remote: Enumerating objects: 647, done.\u001b[K\n",
      "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
      "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
      "remote: Total 647 (delta 9), reused 24 (delta 8), pack-reused 619 (from 1)\u001b[K\n",
      "Receiving objects: 100% (647/647), 42.53 MiB | 15.44 MiB/s, done.\n",
      "Resolving deltas: 100% (304/304), done.\n",
      "Updating files: 100% (108/108), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/isirfanm/yolov7_PTL.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1617,
     "status": "ok",
     "timestamp": 1751275782869,
     "user": {
      "displayName": "Irfan Ismail",
      "userId": "15670942255868779694"
     },
     "user_tz": -420
    },
    "id": "0QY4X6LK_JzL",
    "outputId": "a797d19d-0836-4214-9cfb-a0b59710a5fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Binus/MTI/DL/PTL_component/yolov7_PTL\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/MyDrive/Binus/MTI/DL/PTL_component/yolov7_PTL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 725,
     "status": "ok",
     "timestamp": 1751272731712,
     "user": {
      "displayName": "Irfan Ismail",
      "userId": "15670942255868779694"
     },
     "user_tz": -420
    },
    "id": "9YbxIyIZ_QaM",
    "outputId": "4df1acc2-1d02-43cd-b161-2b9a06ef38fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Branch 'ptl' set up to track remote branch 'ptl' from 'origin'.\n",
      "Switched to a new branch 'ptl'\n"
     ]
    }
   ],
   "source": [
    "!git checkout ptl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2501,
     "status": "ok",
     "timestamp": 1751276541364,
     "user": {
      "displayName": "Irfan Ismail",
      "userId": "15670942255868779694"
     },
     "user_tz": -420
    },
    "id": "qJnQS6mg_QYd",
    "outputId": "b5322838-81d9-44fa-b841-af9110112f6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Enumerating objects: 9, done.\u001b[K\n",
      "remote: Counting objects:  11% (1/9)\u001b[K\rremote: Counting objects:  22% (2/9)\u001b[K\rremote: Counting objects:  33% (3/9)\u001b[K\rremote: Counting objects:  44% (4/9)\u001b[K\rremote: Counting objects:  55% (5/9)\u001b[K\rremote: Counting objects:  66% (6/9)\u001b[K\rremote: Counting objects:  77% (7/9)\u001b[K\rremote: Counting objects:  88% (8/9)\u001b[K\rremote: Counting objects: 100% (9/9)\u001b[K\rremote: Counting objects: 100% (9/9), done.\u001b[K\n",
      "remote: Compressing objects: 100% (1/1)\u001b[K\rremote: Compressing objects: 100% (1/1), done.\u001b[K\n",
      "remote: Total 5 (delta 4), reused 5 (delta 4), pack-reused 0 (from 0)\u001b[K\n",
      "Unpacking objects: 100% (5/5), 425 bytes | 3.00 KiB/s, done.\n",
      "From https://github.com/isirfanm/yolov7_PTL\n",
      "   7ee64ce..3a7edaa  ptl        -> origin/ptl\n",
      "Updating 7ee64ce..3a7edaa\n",
      "Fast-forward\n",
      " train.py          | 4 \u001b[32m++\u001b[m\u001b[31m--\u001b[m\n",
      " utils/datasets.py | 2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " 2 files changed, 3 insertions(+), 3 deletions(-)\n"
     ]
    }
   ],
   "source": [
    "!git pull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YhgdGo5N31S4"
   },
   "source": [
    "# Install Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Al1kam5x35Kc"
   },
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 236403,
     "status": "ok",
     "timestamp": 1751276094671,
     "user": {
      "displayName": "Irfan Ismail",
      "userId": "15670942255868779694"
     },
     "user_tz": -420
    },
    "id": "3fYoHwhAuQzL",
    "outputId": "f4818b4a-a1ee-47a3-ace4-c5fc3edc9bc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (3.10.0)\n",
      "Collecting numpy<1.24.0,>=1.18.5 (from -r requirements.txt (line 5))\n",
      "  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (4.11.0.86)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (11.2.1)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (1.15.3)\n",
      "Collecting torchvision<0.20 (from -r requirements.txt (line 12))\n",
      "  Downloading torchvision-0.19.1-cp311-cp311-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 13)) (4.67.1)\n",
      "Collecting protobuf<3.20.0 (from -r requirements.txt (line 14))\n",
      "  Downloading protobuf-3.19.6-py2.py3-none-any.whl.metadata (828 bytes)\n",
      "Requirement already satisfied: tensorboard>=2.4.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 17)) (2.18.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 21)) (2.2.2)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 22)) (0.13.2)\n",
      "Requirement already satisfied: ipython in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 34)) (7.34.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 35)) (5.9.5)\n",
      "Collecting thop (from -r requirements.txt (line 36))\n",
      "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (2.9.0.post0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (2025.6.15)\n",
      "Collecting torch==2.4.1 (from torchvision<0.20->-r requirements.txt (line 12))\n",
      "  Downloading torch-2.4.1-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.4.1->torchvision<0.20->-r requirements.txt (line 12)) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.1->torchvision<0.20->-r requirements.txt (line 12)) (4.14.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.4.1->torchvision<0.20->-r requirements.txt (line 12)) (1.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.4.1->torchvision<0.20->-r requirements.txt (line 12)) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.1->torchvision<0.20->-r requirements.txt (line 12)) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.4.1->torchvision<0.20->-r requirements.txt (line 12)) (2025.3.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.1->torchvision<0.20->-r requirements.txt (line 12))\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.1->torchvision<0.20->-r requirements.txt (line 12))\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.1->torchvision<0.20->-r requirements.txt (line 12))\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.1->torchvision<0.20->-r requirements.txt (line 12))\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.1->torchvision<0.20->-r requirements.txt (line 12))\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.1->torchvision<0.20->-r requirements.txt (line 12))\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.1->torchvision<0.20->-r requirements.txt (line 12))\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.1->torchvision<0.20->-r requirements.txt (line 12))\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.1->torchvision<0.20->-r requirements.txt (line 12))\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.1->torchvision<0.20->-r requirements.txt (line 12))\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.1->torchvision<0.20->-r requirements.txt (line 12))\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.0.0 (from torch==2.4.1->torchvision<0.20->-r requirements.txt (line 12))\n",
      "  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.1->torchvision<0.20->-r requirements.txt (line 12)) (12.5.82)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (1.73.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (3.8.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (75.2.0)\n",
      "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (1.17.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (3.1.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 21)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 21)) (2025.2)\n",
      "Collecting jedi>=0.16 (from ipython->-r requirements.txt (line 34))\n",
      "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython->-r requirements.txt (line 34)) (4.4.2)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython->-r requirements.txt (line 34)) (0.7.5)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython->-r requirements.txt (line 34)) (5.7.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython->-r requirements.txt (line 34)) (3.0.51)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython->-r requirements.txt (line 34)) (2.19.2)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython->-r requirements.txt (line 34)) (0.2.0)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython->-r requirements.txt (line 34)) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython->-r requirements.txt (line 34)) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython->-r requirements.txt (line 34)) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython->-r requirements.txt (line 34)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->-r requirements.txt (line 34)) (0.2.13)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.4.1->-r requirements.txt (line 17)) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.4.1->torchvision<0.20->-r requirements.txt (line 12)) (1.3.0)\n",
      "Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.19.1-cp311-cp311-manylinux1_x86_64.whl (7.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.4.1-cp311-cp311-manylinux1_x86_64.whl (797.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.1/797.1 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-3.19.6-py2.py3-none-any.whl (162 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
      "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, protobuf, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, jedi, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchvision, thop\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.2.0\n",
      "    Uninstalling triton-3.2.0:\n",
      "      Successfully uninstalled triton-3.2.0\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.29.5\n",
      "    Uninstalling protobuf-5.29.5:\n",
      "      Successfully uninstalled protobuf-5.29.5\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
      "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
      "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.6.0+cu124\n",
      "    Uninstalling torch-2.6.0+cu124:\n",
      "      Successfully uninstalled torch-2.6.0+cu124\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.21.0+cu124\n",
      "    Uninstalling torchvision-0.21.0+cu124:\n",
      "      Successfully uninstalled torchvision-0.21.0+cu124\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
      "google-cloud-datastore 2.21.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
      "google-cloud-iam 2.19.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
      "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
      "googleapis-common-protos 1.70.0 requires protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
      "google-cloud-resource-manager 1.14.2 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
      "google-cloud-bigquery-storage 2.32.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
      "bigframes 2.8.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
      "ydf 0.12.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 3.19.6 which is incompatible.\n",
      "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
      "pymc 5.23.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
      "google-cloud-firestore 2.21.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
      "tensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.19.6 which is incompatible.\n",
      "google-cloud-bigquery-connection 1.18.3 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
      "xarray 2025.3.1 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.23.5 which is incompatible.\n",
      "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.4.1 which is incompatible.\n",
      "grpc-google-iam-v1 0.14.2 requires protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
      "albucore 0.0.24 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
      "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.19.6 which is incompatible.\n",
      "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.23.5 which is incompatible.\n",
      "tensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\n",
      "google-cloud-language 2.17.2 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
      "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
      "albumentations 2.0.8 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
      "google-cloud-dataproc 5.20.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
      "db-dtypes 1.4.3 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
      "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n",
      "google-cloud-functions 1.20.4 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
      "tensorflow-datasets 4.9.9 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
      "xarray-einstats 0.9.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
      "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
      "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
      "google-cloud-spanner 3.55.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
      "blosc2 3.5.0 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n",
      "google-cloud-aiplatform 1.99.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
      "google-cloud-translate 3.20.3 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed jedi-0.19.2 numpy-1.23.5 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 protobuf-3.19.6 thop-0.1.1.post2209072238 torch-2.4.1 torchvision-0.19.1 triton-3.0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "de73c68bfeae4a68a553c0afecb9cf16",
       "pip_warning": {
        "packages": [
         "google",
         "numpy"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 9489,
     "status": "ok",
     "timestamp": 1751275178219,
     "user": {
      "displayName": "Irfan Ismail",
      "userId": "15670942255868779694"
     },
     "user_tz": -420
    },
    "id": "1QAxLcbwIhZy",
    "outputId": "bf39da6a-cb3a-48d2-8bc5-8b1710c93808"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting tensorboard==2.4.1\n",
      "  Downloading tensorboard-2.4.1-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.4.1) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.4.1) (1.73.0)\n",
      "Collecting google-auth<2,>=1.6.3 (from tensorboard==2.4.1)\n",
      "  Downloading google_auth-1.35.0-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard==2.4.1)\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.4.1) (3.8.2)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.4.1) (1.23.5)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.4.1) (4.21.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.4.1) (2.32.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.4.1) (75.2.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.4.1) (1.17.0)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard==2.4.1)\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl.metadata (873 bytes)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.4.1) (3.1.3)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.4.1) (0.45.1)\n",
      "Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard==2.4.1)\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.4.1) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.4.1) (4.9.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.4.1) (2.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard==2.4.1) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard==2.4.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard==2.4.1) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard==2.4.1) (2025.6.15)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=0.11.15->tensorboard==2.4.1) (3.0.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard==2.4.1) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.4.1) (3.3.1)\n",
      "Downloading tensorboard-2.4.1-py3-none-any.whl (10.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.6/10.6 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.9/152.9 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: tensorboard-plugin-wit, cachetools, google-auth, google-auth-oauthlib, tensorboard\n",
      "  Attempting uninstall: cachetools\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: cachetools 5.5.2\n",
      "    Uninstalling cachetools-5.5.2:\n",
      "      Successfully uninstalled cachetools-5.5.2\n",
      "  Attempting uninstall: google-auth\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: google-auth 2.38.0\n",
      "    Uninstalling google-auth-2.38.0:\n",
      "      Successfully uninstalled google-auth-2.38.0\n",
      "  Attempting uninstall: google-auth-oauthlib\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: google-auth-oauthlib 1.2.2\n",
      "    Uninstalling google-auth-oauthlib-1.2.2:\n",
      "      Successfully uninstalled google-auth-oauthlib-1.2.2\n",
      "  Attempting uninstall: tensorboard\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: tensorboard 2.18.0\n",
      "    Uninstalling tensorboard-2.18.0:\n",
      "      Successfully uninstalled tensorboard-2.18.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 1.35.0 which is incompatible.\n",
      "google-cloud-storage 2.19.0 requires google-auth<3.0dev,>=2.26.1, but you have google-auth 1.35.0 which is incompatible.\n",
      "google-cloud-datastore 2.21.0 requires google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1, but you have google-auth 1.35.0 which is incompatible.\n",
      "google-cloud-datastore 2.21.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 4.21.2 which is incompatible.\n",
      "google-cloud-bigquery 3.34.0 requires google-auth<3.0.0,>=2.14.1, but you have google-auth 1.35.0 which is incompatible.\n",
      "google-cloud-iam 2.19.1 requires google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1, but you have google-auth 1.35.0 which is incompatible.\n",
      "google-cloud-iam 2.19.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 4.21.2 which is incompatible.\n",
      "google-genai 1.21.1 requires google-auth<3.0.0,>=2.14.1, but you have google-auth 1.35.0 which is incompatible.\n",
      "google-cloud-resource-manager 1.14.2 requires google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1, but you have google-auth 1.35.0 which is incompatible.\n",
      "google-cloud-resource-manager 1.14.2 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 4.21.2 which is incompatible.\n",
      "google-cloud-bigquery-storage 2.32.0 requires google-auth<3.0.0,>=2.14.1, but you have google-auth 1.35.0 which is incompatible.\n",
      "google-cloud-bigquery-storage 2.32.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 4.21.2 which is incompatible.\n",
      "bigframes 2.8.0 requires google-auth<3.0,>=2.15.0, but you have google-auth 1.35.0 which is incompatible.\n",
      "bigframes 2.8.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
      "pymc 5.23.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
      "google-cloud-firestore 2.21.0 requires google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1, but you have google-auth 1.35.0 which is incompatible.\n",
      "google-cloud-firestore 2.21.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0dev,>=3.20.2, but you have protobuf 4.21.2 which is incompatible.\n",
      "google-cloud-bigquery-connection 1.18.3 requires google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1, but you have google-auth 1.35.0 which is incompatible.\n",
      "google-cloud-bigquery-connection 1.18.3 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 4.21.2 which is incompatible.\n",
      "google-api-core 2.25.1 requires google-auth<3.0.0,>=2.14.1, but you have google-auth 1.35.0 which is incompatible.\n",
      "google-api-core 2.25.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5, but you have protobuf 4.21.2 which is incompatible.\n",
      "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.23.5 which is incompatible.\n",
      "tensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 4.21.2 which is incompatible.\n",
      "tensorflow 2.18.0 requires tensorboard<2.19,>=2.18, but you have tensorboard 2.4.1 which is incompatible.\n",
      "google-cloud-language 2.17.2 requires google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1, but you have google-auth 1.35.0 which is incompatible.\n",
      "google-cloud-language 2.17.2 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 4.21.2 which is incompatible.\n",
      "pandas-gbq 0.29.1 requires google-auth>=2.13.0, but you have google-auth 1.35.0 which is incompatible.\n",
      "pandas-gbq 0.29.1 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
      "google-cloud-dataproc 5.20.0 requires google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1, but you have google-auth 1.35.0 which is incompatible.\n",
      "google-cloud-dataproc 5.20.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 4.21.2 which is incompatible.\n",
      "google-cloud-functions 1.20.4 requires google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1, but you have google-auth 1.35.0 which is incompatible.\n",
      "google-cloud-functions 1.20.4 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 4.21.2 which is incompatible.\n",
      "google-generativeai 0.8.5 requires google-auth>=2.15.0, but you have google-auth 1.35.0 which is incompatible.\n",
      "google-ai-generativelanguage 0.6.15 requires google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1, but you have google-auth 1.35.0 which is incompatible.\n",
      "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 4.21.2 which is incompatible.\n",
      "google-cloud-spanner 3.55.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 4.21.2 which is incompatible.\n",
      "google-cloud-aiplatform 1.99.0 requires google-auth<3.0.0,>=2.14.1, but you have google-auth 1.35.0 which is incompatible.\n",
      "google-cloud-aiplatform 1.99.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 4.21.2 which is incompatible.\n",
      "google-cloud-translate 3.20.3 requires google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1, but you have google-auth 1.35.0 which is incompatible.\n",
      "google-cloud-translate 3.20.3 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 4.21.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed cachetools-4.2.4 google-auth-1.35.0 google-auth-oauthlib-0.4.6 tensorboard-2.4.1 tensorboard-plugin-wit-1.8.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "dc5ae3310a424652abf59fe2e910a65c",
       "pip_warning": {
        "packages": [
         "google"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install \"tensorboard==2.4.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 53140,
     "status": "ok",
     "timestamp": 1751275572363,
     "user": {
      "displayName": "Irfan Ismail",
      "userId": "15670942255868779694"
     },
     "user_tz": -420
    },
    "id": "pevc1iMIDJtm",
    "outputId": "b0c2214f-31a4-48de-fbb1-ec23381cf2b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torchvision<0.20 in /usr/local/lib/python3.11/dist-packages (0.19.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision<0.20) (1.23.5)\n",
      "Collecting torch==2.4.1 (from torchvision<0.20)\n",
      "  Using cached torch-2.4.1-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision<0.20) (11.2.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.4.1->torchvision<0.20) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.1->torchvision<0.20) (4.14.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.4.1->torchvision<0.20) (1.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.4.1->torchvision<0.20) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.1->torchvision<0.20) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.4.1->torchvision<0.20) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.1->torchvision<0.20) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.1->torchvision<0.20) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.1->torchvision<0.20) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.1->torchvision<0.20) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.1->torchvision<0.20) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.1->torchvision<0.20) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.1->torchvision<0.20) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.1->torchvision<0.20) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.1->torchvision<0.20) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.1->torchvision<0.20) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.1->torchvision<0.20) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.1->torchvision<0.20) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.1->torchvision<0.20) (12.6.85)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.4.1->torchvision<0.20) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.4.1->torchvision<0.20) (1.3.0)\n",
      "Using cached torch-2.4.1-cp311-cp311-manylinux1_x86_64.whl (797.1 MB)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: torch\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch\n"
     ]
    }
   ],
   "source": [
    "!pip install \"torchvision<0.20\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "executionInfo": {
     "elapsed": 22821,
     "status": "ok",
     "timestamp": 1751276682108,
     "user": {
      "displayName": "Irfan Ismail",
      "userId": "15670942255868779694"
     },
     "user_tz": -420
    },
    "id": "HOUhto5IONfv",
    "outputId": "3b9798f7-dc5d-468f-f2ff-7110ecafe9aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.26.4\n",
      "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.23.5\n",
      "    Uninstalling numpy-1.23.5:\n",
      "      Successfully uninstalled numpy-1.23.5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ydf 0.12.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 3.19.6 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "tensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\n",
      "tensorflow-datasets 4.9.9 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
      "google-cloud-aiplatform 1.99.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.4\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "c0c35ca86a2940e5bad4307fdc1ae3d8",
       "pip_warning": {
        "packages": [
         "numpy"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install \"numpy==1.26.4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KesxrB_dXcEG"
   },
   "outputs": [],
   "source": [
    "!pip freeze > requirements_freeze.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6odatd84BW4"
   },
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0lu-VDEWStog"
   },
   "source": [
    "## Go to working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1751285704346,
     "user": {
      "displayName": "Irfan Ismail",
      "userId": "15670942255868779694"
     },
     "user_tz": -420
    },
    "id": "rohkx-_SuV1y",
    "outputId": "d7178231-f6a6-495f-b3d8-a02432dfd8bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Binus/MTI/DL/PTL_component/yolov7_PTL\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/MyDrive/Binus/MTI/DL/PTL_component/yolov7_PTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWhDliQTv6QG"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2412,
     "status": "ok",
     "timestamp": 1751283927206,
     "user": {
      "displayName": "Irfan Ismail",
      "userId": "15670942255868779694"
     },
     "user_tz": -420
    },
    "id": "KpAmJ4I0xncP"
   },
   "outputs": [],
   "source": [
    "# prompt: train yolov7-tiny model with grid search and early stop\n",
    "import yaml\n",
    "import wandb\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "# Define your dataset configuration file path\n",
    "data_yaml_path = 'data/furnas.yaml' # Make sure this path is correct and the file exists\n",
    "\n",
    "# Define the base configuration for yolov7-tiny\n",
    "config_file = 'cfg/training/yolov7-tiny-custom.yaml' # Adjust path if necessary\n",
    "\n",
    "# Define the hyperparameter\n",
    "hyperparameter_file = 'data/hyp.scratch.tiny.custom.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/irfan/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mis-irfan-moh\u001b[0m (\u001b[33mis-irfan-moh-binus-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb \n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHwUj3cQtBMJ"
   },
   "source": [
    "### Single Train (Scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3380,
     "status": "ok",
     "timestamp": 1751283931608,
     "user": {
      "displayName": "Irfan Ismail",
      "userId": "15670942255868779694"
     },
     "user_tz": -420
    },
    "id": "vLM4vBRb12KY",
    "outputId": "5a3a626b-f98a-4bb1-9c67-78cd05ccd735"
   },
   "outputs": [],
   "source": [
    "!python train.py --device '0' --workers 16 --batch-size 64 --epochs 300  --data \"data/furnas.yaml\" --cfg \"yolov7-tiny-custom.yaml\"  --name yolov7-custom --hyp \"data/hyp.scratch.tiny.custom.yaml\" --weights ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_fAycoFtFfI"
   },
   "source": [
    "### Single Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qxwdder0wK9V",
    "outputId": "e0fcabad-bc5b-4335-d911-51ddb8b5c947"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOR 🚀 v0.1-136-gbfbe327 torch 2.7.1+cu126 CPU\n",
      "\n",
      "Namespace(weights='./yolov7-tiny.pt', cfg='./cfg/training/yolov7-tiny-custom.yaml', data='data/furnas.yaml', hyp='data/hyp.scratch.tiny.custom.yaml', epochs=300, batch_size=32, img_size=[640, 640], rect=False, resume=False, nosave=False, notest=False, noautoanchor=False, evolve=False, bucket='', cache_images=False, image_weights=False, device='cpu', multi_scale=False, single_cls=False, adam=True, sync_bn=False, local_rank=-1, workers=8, project='runs/train', entity=None, name='yolov7-tiny-ptl', exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, upload_dataset=False, bbox_interval=-1, save_period=-1, artifact_alias='latest', freeze=[0], v5_metric=False, world_size=1, global_rank=-1, save_dir='runs/train/yolov7-tiny-ptl7', total_batch_size=32)\n",
      "\u001b[34m\u001b[1mtensorboard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.001, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.05, copy_paste=0.0, paste_in=0.05, loss_ota=1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mis-irfan-moh\u001b[0m (\u001b[33mis-irfan-moh-binus-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m creating run (0.3s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m creating run (0.3s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m creating run (0.3s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m creating run (0.3s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.20.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/irfan/workspace/home/Kuliah/tesis_PTL/yolov7_PTL/wandb/run-20250630_204043-918zcici\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33myolov7-tiny-ptl7\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/is-irfan-moh-binus-university/YOLOR\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/is-irfan-moh-binus-university/YOLOR/runs/918zcici\u001b[0m\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1       928  models.common.Conv                      [3, 32, 3, 2, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      "  2                -1  1      2112  models.common.Conv                      [64, 32, 1, 1, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      "  3                -2  1      2112  models.common.Conv                      [64, 32, 1, 1, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      "  4                -1  1      9280  models.common.Conv                      [32, 32, 3, 1, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      "  5                -1  1      9280  models.common.Conv                      [32, 32, 3, 1, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      "  6  [-1, -2, -3, -4]  1         0  models.common.Concat                    [1]                           \n",
      "  7                -1  1      8320  models.common.Conv                      [128, 64, 1, 1, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      "  8                -1  1         0  models.common.MP                        []                            \n",
      "  9                -1  1      4224  models.common.Conv                      [64, 64, 1, 1, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      " 10                -2  1      4224  models.common.Conv                      [64, 64, 1, 1, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      " 11                -1  1     36992  models.common.Conv                      [64, 64, 3, 1, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      " 12                -1  1     36992  models.common.Conv                      [64, 64, 3, 1, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      " 13  [-1, -2, -3, -4]  1         0  models.common.Concat                    [1]                           \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      " 15                -1  1         0  models.common.MP                        []                            \n",
      " 16                -1  1     16640  models.common.Conv                      [128, 128, 1, 1, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      " 17                -2  1     16640  models.common.Conv                      [128, 128, 1, 1, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 1, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      " 19                -1  1    147712  models.common.Conv                      [128, 128, 3, 1, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      " 20  [-1, -2, -3, -4]  1         0  models.common.Concat                    [1]                           \n",
      " 21                -1  1    131584  models.common.Conv                      [512, 256, 1, 1, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      " 22                -1  1         0  models.common.MP                        []                            \n",
      " 23                -1  1     66048  models.common.Conv                      [256, 256, 1, 1, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      " 24                -2  1     66048  models.common.Conv                      [256, 256, 1, 1, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      " 25                -1  1    590336  models.common.Conv                      [256, 256, 3, 1, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      " 26                -1  1    590336  models.common.Conv                      [256, 256, 3, 1, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      " 27  [-1, -2, -3, -4]  1         0  models.common.Concat                    [1]                           \n",
      " 28                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      " 29                -1  1    131584  models.common.Conv                      [512, 256, 1, 1, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      " 30                -2  1    131584  models.common.Conv                      [512, 256, 1, 1, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      " 31                -1  1         0  models.common.SP                        [5]                           \n",
      " 32                -2  1         0  models.common.SP                        [9]                           \n",
      " 33                -3  1         0  models.common.SP                        [13]                          \n",
      " 34  [-1, -2, -3, -4]  1         0  models.common.Concat                    [1]                           \n",
      " 35                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      " 36          [-1, -7]  1         0  models.common.Concat                    [1]                           \n",
      " 37                -1  1    131584  models.common.Conv                      [512, 256, 1, 1, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      " 38                -1  1     33024  models.common.Conv                      [256, 128, 1, 1, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      " 39                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 40                21  1     33024  models.common.Conv                      [256, 128, 1, 1, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      " 41          [-1, -2]  1         0  models.common.Concat                    [1]                           \n",
      " 42                -1  1     16512  models.common.Conv                      [256, 64, 1, 1, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      " 43                -2  1     16512  models.common.Conv                      [256, 64, 1, 1, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      " 44                -1  1     36992  models.common.Conv                      [64, 64, 3, 1, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      " 45                -1  1     36992  models.common.Conv                      [64, 64, 3, 1, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      " 46  [-1, -2, -3, -4]  1         0  models.common.Concat                    [1]                           \n",
      " 47                -1  1     33024  models.common.Conv                      [256, 128, 1, 1, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      " 48                -1  1      8320  models.common.Conv                      [128, 64, 1, 1, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      " 49                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 50                14  1      8320  models.common.Conv                      [128, 64, 1, 1, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      " 51          [-1, -2]  1         0  models.common.Concat                    [1]                           \n",
      " 52                -1  1      4160  models.common.Conv                      [128, 32, 1, 1, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      " 53                -2  1      4160  models.common.Conv                      [128, 32, 1, 1, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      " 54                -1  1      9280  models.common.Conv                      [32, 32, 3, 1, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      " 55                -1  1      9280  models.common.Conv                      [32, 32, 3, 1, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      " 56  [-1, -2, -3, -4]  1         0  models.common.Concat                    [1]                           \n",
      " 57                -1  1      8320  models.common.Conv                      [128, 64, 1, 1, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      " 58                -1  1     73984  models.common.Conv                      [64, 128, 3, 2, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      " 59          [-1, 47]  1         0  models.common.Concat                    [1]                           \n",
      " 60                -1  1     16512  models.common.Conv                      [256, 64, 1, 1, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      " 61                -2  1     16512  models.common.Conv                      [256, 64, 1, 1, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      " 62                -1  1     36992  models.common.Conv                      [64, 64, 3, 1, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      " 63                -1  1     36992  models.common.Conv                      [64, 64, 3, 1, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      " 64  [-1, -2, -3, -4]  1         0  models.common.Concat                    [1]                           \n",
      " 65                -1  1     33024  models.common.Conv                      [256, 128, 1, 1, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      " 66                -1  1    295424  models.common.Conv                      [128, 256, 3, 2, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      " 67          [-1, 37]  1         0  models.common.Concat                    [1]                           \n",
      " 68                -1  1     65792  models.common.Conv                      [512, 128, 1, 1, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      " 69                -2  1     65792  models.common.Conv                      [512, 128, 1, 1, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      " 70                -1  1    147712  models.common.Conv                      [128, 128, 3, 1, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      " 71                -1  1    147712  models.common.Conv                      [128, 128, 3, 1, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      " 72  [-1, -2, -3, -4]  1         0  models.common.Concat                    [1]                           \n",
      " 73                -1  1    131584  models.common.Conv                      [512, 256, 1, 1, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      " 74                57  1     73984  models.common.Conv                      [64, 128, 3, 1, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      " 75                65  1    295424  models.common.Conv                      [128, 256, 3, 1, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      " 76                73  1   1180672  models.common.Conv                      [256, 512, 3, 1, None, 1, LeakyReLU(negative_slope=0.1015625)]\n",
      " 77      [74, 75, 76]  1     43152  models.yolo.Detect                      [11, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "/home/irfan/.pyenv/versions/yolov7/lib/python3.9/site-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4314.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Model Summary: 255 layers, 6041008 parameters, 6041008 gradients, 13.3 GFLOPS\n",
      "\n",
      "Transferred 330/338 items from ./yolov7-tiny.pt\n",
      "Scaled weight_decay = 0.0005\n",
      "Optimizer groups: 58 .bias, 58 conv.weight, 55 other\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '../furnas_dataset_v0.07/split_dataset/train/labels' images and \u001b[0m\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label ../furnas_dataset_v0.07/split_dataset/train/images/LTIABIVP1_T0210.ts-16182#032.jpg: duplicate labels\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '../furnas_dataset_v0.07/split_dataset/train/labels' images and \u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: ../furnas_dataset_v0.07/split_dataset/train/labels.cache\n",
      "/home/irfan/.pyenv/versions/yolov7/lib/python3.9/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '../furnas_dataset_v0.07/split_dataset/val/labels' images and labe\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: ../furnas_dataset_v0.07/split_dataset/val/labels.cache\n",
      "\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mAnalyzing anchors... anchors/target = 3.41, Best Possible Recall (BPR) = 0.9908\n",
      "/home/irfan/workspace/home/Kuliah/tesis_PTL/yolov7_PTL/train.py:299: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = amp.GradScaler(enabled=cuda)\n",
      "Image sizes 640 train, 640 test\n",
      "Using 8 dataloader workers\n",
      "Logging results to runs/train/yolov7-tiny-ptl7\n",
      "Starting training for 300 epochs...\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
      "  0%|                                                   | 0/138 [00:00<?, ?it/s]/home/irfan/workspace/home/Kuliah/tesis_PTL/yolov7_PTL/train.py:360: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(enabled=cuda):\n",
      "     0/299        0G   0.09481   0.02509   0.07182    0.1917       155       640^C\n",
      "Process Process-13:\n",
      "Process Process-12:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/multiprocessing/process.py\", line 318, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/multiprocessing/util.py\", line 360, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/multiprocessing/queues.py\", line 199, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/threading.py\", line 1060, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/threading.py\", line 1080, in _wait_for_tstate_lock\n",
      "    if lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n",
      "Process Process-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/multiprocessing/process.py\", line 318, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/multiprocessing/util.py\", line 360, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/multiprocessing/queues.py\", line 199, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/threading.py\", line 1060, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/threading.py\", line 1080, in _wait_for_tstate_lock\n",
      "    if lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n",
      "Process Process-15:\n",
      "Process Process-16:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/multiprocessing/process.py\", line 318, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/multiprocessing/util.py\", line 360, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/multiprocessing/queues.py\", line 199, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/threading.py\", line 1060, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/threading.py\", line 1080, in _wait_for_tstate_lock\n",
      "    if lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n",
      "Process Process-10:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/multiprocessing/process.py\", line 318, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/multiprocessing/process.py\", line 318, in _bootstrap\n",
      "    util._exit_function()\n",
      "Process Process-11:\n",
      "Process Process-14:\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/multiprocessing/util.py\", line 360, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/multiprocessing/queues.py\", line 199, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/threading.py\", line 1060, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/threading.py\", line 1080, in _wait_for_tstate_lock\n",
      "    if lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/multiprocessing/process.py\", line 318, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/multiprocessing/util.py\", line 360, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/multiprocessing/queues.py\", line 199, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/threading.py\", line 1060, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/threading.py\", line 1080, in _wait_for_tstate_lock\n",
      "    if lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/multiprocessing/process.py\", line 318, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/multiprocessing/util.py\", line 360, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/multiprocessing/queues.py\", line 199, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/threading.py\", line 1060, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/threading.py\", line 1080, in _wait_for_tstate_lock\n",
      "    if lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/multiprocessing/process.py\", line 318, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/multiprocessing/util.py\", line 360, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/multiprocessing/util.py\", line 360, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/multiprocessing/queues.py\", line 199, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/threading.py\", line 1060, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/threading.py\", line 1080, in _wait_for_tstate_lock\n",
      "    if lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/multiprocessing/queues.py\", line 199, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/threading.py\", line 1060, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/threading.py\", line 1080, in _wait_for_tstate_lock\n",
      "    if lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n",
      "     0/299        0G   0.09481   0.02509   0.07182    0.1917       155       640\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/irfan/workspace/home/Kuliah/tesis_PTL/yolov7_PTL/train.py\", line 757, in <module>\n",
      "    train(hyp, opt, device, tb_writer)\n",
      "  File \"/home/irfan/workspace/home/Kuliah/tesis_PTL/yolov7_PTL/train.py\", line 361, in train\n",
      "    pred = model(imgs)  # forward\n",
      "  File \"/home/irfan/.pyenv/versions/yolov7/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/irfan/.pyenv/versions/yolov7/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/irfan/workspace/home/Kuliah/tesis_PTL/yolov7_PTL/models/yolo.py\", line 599, in forward\n",
      "    return self.forward_once(x, profile)  # single-scale inference, train\n",
      "  File \"/home/irfan/workspace/home/Kuliah/tesis_PTL/yolov7_PTL/models/yolo.py\", line 625, in forward_once\n",
      "    x = m(x)  # run\n",
      "  File \"/home/irfan/.pyenv/versions/yolov7/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/irfan/.pyenv/versions/yolov7/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/irfan/workspace/home/Kuliah/tesis_PTL/yolov7_PTL/models/common.py\", line 108, in forward\n",
      "    return self.act(self.bn(self.conv(x)))\n",
      "  File \"/home/irfan/.pyenv/versions/yolov7/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/irfan/.pyenv/versions/yolov7/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/irfan/.pyenv/versions/yolov7/lib/python3.9/site-packages/torch/nn/modules/conv.py\", line 554, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "  File \"/home/irfan/.pyenv/versions/yolov7/lib/python3.9/site-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n",
      "    return F.conv2d(\n",
      "KeyboardInterrupt\n",
      "Error in atexit._run_exitfuncs:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/irfan/.pyenv/versions/yolov7/lib/python3.9/site-packages/wandb/sdk/lib/sock_client.py\", line 151, in _send_message\n",
      "    self._sendall_with_error_handle(header + data)\n",
      "  File \"/home/irfan/.pyenv/versions/yolov7/lib/python3.9/site-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\n",
      "    sent = self._sock.send(data)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Error in atexit._run_exitfuncs:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/irfan/.pyenv/versions/3.9.21/lib/python3.9/multiprocessing/popen_fork.py\", line 27, in poll\n",
      "    pid, sts = os.waitpid(self.pid, flag)\n",
      "  File \"/home/irfan/.pyenv/versions/yolov7/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py\", line 73, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 11738) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.\n"
     ]
    }
   ],
   "source": [
    "!python train.py --device 'cpu' --workers 8 --batch-size 32 --epochs 300  --data \"data/furnas.yaml\" --cfg \"yolov7-tiny-custom.yaml\"  --name yolov7-tiny-ptl --hyp \"data/hyp.scratch.tiny.custom.yaml\" --weights './yolov7-tiny.pt' --adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0ly_OfStLDe"
   },
   "source": [
    "### Training with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eab9ZPq7xrhN"
   },
   "outputs": [],
   "source": [
    "# Define the grid search parameters\n",
    "# Example grid search: varying batch size and learning rate\n",
    "grid_params = {\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'epochs': [200], # Keep epochs relatively low for grid search iterations\n",
    "    'img_size': [640],\n",
    "    'cfg': [config_file],\n",
    "    'data': [data_yaml_path],\n",
    "    'weights': [''], # Start training from scratch or a pretrained tiny model\n",
    "    'hyp': [hyperparameter_file], # Use the default hyperparameter file for tiny\n",
    "    'name': ['yolov7-tiny-grid-search'],\n",
    "    'cache-images': [True],\n",
    "    'patience': [10] # Early stopping patience\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1aWyrFHdx0qv"
   },
   "outputs": [],
   "source": [
    "# Function to train a single model configuration\n",
    "def train_model(params):\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Starting training with parameters:\")\n",
    "    print(json.dumps(params, indent=2))\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "    # Construct the training command\n",
    "    command = [\n",
    "        'python', 'train.py',\n",
    "        '--img-size', str(params['img_size']),\n",
    "        '--batch-size', str(params['batch_size']),\n",
    "        '--epochs', str(params['epochs']),\n",
    "        '--data', params['data'],\n",
    "        '--cfg', params['cfg'],\n",
    "        '--weights', params['weights'],\n",
    "        '--hyp', params['hyp'],\n",
    "        '--name', params['name'],\n",
    "        # '--cache-images',\n",
    "        # '--patience', str(params['patience'])\n",
    "    ]\n",
    "\n",
    "    # Add --sync-bn if needed (for multi-GPU training, not typically needed in Colab)\n",
    "    # command.append('--sync-bn')\n",
    "\n",
    "    # Add --device if you want to specify GPU or CPU\n",
    "    command.extend(['--device', '0']) # Use GPU 0\n",
    "    # command.extend(['--device', 'cpu']) # Use CPU\n",
    "\n",
    "    # Add --bbox_interval and --image_weights if using wandb and logging specific things\n",
    "    # command.extend(['--bbox_interval', '1', '--image_weights'])\n",
    "\n",
    "    try:\n",
    "        # Execute the training command\n",
    "        process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        stdout, stderr = process.communicate()\n",
    "\n",
    "        print(\"STDOUT:\")\n",
    "        print(stdout.decode('utf-8'))\n",
    "        print(\"STDERR:\")\n",
    "        print(stderr.decode('utf-8'))\n",
    "\n",
    "        if process.returncode != 0:\n",
    "            print(f\"Training failed with return code {process.returncode}\")\n",
    "        else:\n",
    "            print(\"Training completed successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during training: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1751264518524,
     "user": {
      "displayName": "Irfan Ismail",
      "userId": "15670942255868779694"
     },
     "user_tz": -420
    },
    "id": "2eqqhqDjx3hX",
    "outputId": "eb407937-c2e0-40b5-c63b-730b1b4c83b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total grid search runs: 3\n"
     ]
    }
   ],
   "source": [
    "# Simple grid search implementation\n",
    "# This is a basic implementation. For more complex grid searches,\n",
    "# consider libraries like sklearn.model_selection.ParameterGrid\n",
    "# or tools integrated with experiment trackers like Wandb Sweeps.\n",
    "from itertools import product\n",
    "\n",
    "# Generate combinations of parameters\n",
    "param_combinations = list(product(\n",
    "    grid_params['batch_size'],\n",
    "    grid_params['epochs'],\n",
    "    grid_params['img_size'],\n",
    "    grid_params['cfg'],\n",
    "    grid_params['data'],\n",
    "    grid_params['weights'],\n",
    "    grid_params['hyp'],\n",
    "    grid_params['name'],\n",
    "    # grid_params['cache-images'],\n",
    "    # grid_params['patience']\n",
    "))\n",
    "\n",
    "print(f\"Total grid search runs: {len(param_combinations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 644
    },
    "executionInfo": {
     "elapsed": 213664,
     "status": "error",
     "timestamp": 1751264796839,
     "user": {
      "displayName": "Irfan Ismail",
      "userId": "15670942255868779694"
     },
     "user_tz": -420
    },
    "id": "uuMrfDL2q609",
    "outputId": "33cc3f65-2fc6-48e3-b6db-4672361fc3d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Starting training with parameters:\n",
      "{\n",
      "  \"batch_size\": 16,\n",
      "  \"epochs\": 200,\n",
      "  \"img_size\": 640,\n",
      "  \"cfg\": \"/content/drive/MyDrive/Binus/MTI/DL/PTL_component/yolov7/cfg/training/yolov7-tiny-custom.yaml\",\n",
      "  \"data\": \"/content/drive/MyDrive/Binus/MTI/DL/PTL_component/yolov7/data/furnas.yaml\",\n",
      "  \"weights\": \"\",\n",
      "  \"hyp\": \"/content/drive/MyDrive/Binus/MTI/DL/PTL_component/yolov7/data/hyp.scratch.tiny.custom.yaml\",\n",
      "  \"name\": \"yolov7-tiny-grid-search_0\"\n",
      "}\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-41-2273376302.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# 'patience': combo[9]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     }\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-36-702308597.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Execute the training command\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"STDOUT:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1209\u001b[0;31m                 \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1210\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m                 \u001b[0;31m# https://bugs.python.org/issue25942\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/subprocess.py\u001b[0m in \u001b[0;36m_communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   2113\u001b[0m                             'failed to raise TimeoutExpired.')\n\u001b[1;32m   2114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                     \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run 1 training for each combination\n",
    "for i, combo in enumerate(param_combinations):\n",
    "    params = {\n",
    "        'batch_size': combo[0],\n",
    "        'epochs': combo[1],\n",
    "        'img_size': combo[2],\n",
    "        'cfg': combo[3],\n",
    "        'data': combo[4],\n",
    "        'weights': combo[5],\n",
    "        'hyp': combo[6],\n",
    "        'name': f\"{combo[7]}_{i}\", # Append index to name for unique runs\n",
    "        # 'cache-images': combo[8],\n",
    "        # 'patience': combo[9]\n",
    "    }\n",
    "    train_model(params)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Grid search completed.\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# You can now analyze the results in the 'runs/train' directory or on Wandb if used.\n",
    "# Look for the experiment directories named 'yolov7-tiny-grid-search_X'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ApU53aqx787"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z636rjYy4E_q"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BcESTf8KtgV5"
   },
   "source": [
    "## Go to working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2047,
     "status": "ok",
     "timestamp": 1751283921888,
     "user": {
      "displayName": "Irfan Ismail",
      "userId": "15670942255868779694"
     },
     "user_tz": -420
    },
    "id": "6E8CrvIotaId",
    "outputId": "9866d33a-2504-47d9-c093-2fd476460beb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Binus/MTI/DL/PTL_component/yolov7_PTL\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/MyDrive/Binus/MTI/DL/PTL_component/yolov7_PTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eCU6Y5TRtiB9"
   },
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iHA2SbCk4GLZ"
   },
   "outputs": [],
   "source": [
    "# prompt: evaluate model with test dataset\n",
    "\n",
    "# After training is complete, evaluate the best model on the test set.\n",
    "\n",
    "# Find the path to the best trained weights.\n",
    "# This typically involves looking in the `runs/train` directory for the run\n",
    "# with the best performance on the validation set (e.g., highest mAP@0.5).\n",
    "# You'll need to identify the best run and the path to its 'best.pt' weights file.\n",
    "\n",
    "# Example (you need to replace with the actual path):\n",
    "# Assuming the best model from the grid search was run 0 and saved to runs/train/yolov7-tiny-grid-search_0/weights/best.pt\n",
    "best_weights_path = '/content/drive/MyDrive/Binus/MTI/DL/PTL_component/furnas_dataset_v0.07/yolov7/runs/train/yolov7-tiny-grid-search_0/weights/best.pt' # **UPDATE THIS PATH**\n",
    "\n",
    "# Define the path to the test dataset images and labels\n",
    "test_data_yaml_path = '/content/drive/MyDrive/Binus/MTI/DL/PTL_component/furnas_dataset_v0.07/split_dataset/furnas_dataset.yaml' # Use the same data.yaml, it should point to train, val, and test paths\n",
    "\n",
    "# Define the image size used during training\n",
    "img_size_for_eval = 640 # Should match the img_size used for the best training run\n",
    "\n",
    "# Define the confidence and IoU thresholds for evaluation\n",
    "# These are common values, you might adjust them based on your requirements\n",
    "conf_thres = 0.001  # Object confidence threshold (lower to detect more objects)\n",
    "iou_thres = 0.65    # IoU threshold for NMS and evaluation metrics (higher for stricter matching)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Starting model evaluation on the test set...\")\n",
    "print(f\"Using weights: {best_weights_path}\")\n",
    "print(f\"Using data config: {test_data_yaml_path}\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "# Construct the evaluation command (using detect.py or test.py if available)\n",
    "# YOLOv7 typically uses `test.py` for evaluation on a dataset split.\n",
    "# If you want to detect on individual images and visualize, use `detect.py`.\n",
    "# We'll use `test.py` for standard evaluation metrics (mAP).\n",
    "\n",
    "eval_command = [\n",
    "    'python', 'test.py',\n",
    "    '--img-size', str(img_size_for_eval),\n",
    "    '--data', test_data_yaml_path,\n",
    "    '--weights', best_weights_path,\n",
    "    '--batch-size', '32', # Use a batch size that fits in memory\n",
    "    '--conf-thres', str(conf_thres),\n",
    "    '--iou-thres', str(iou_thres),\n",
    "    '--task', 'test', # Specify that we are evaluating the 'test' split\n",
    "    '--name', 'yolov7-tiny-test-eval' # Name for the evaluation run directory\n",
    "]\n",
    "\n",
    "# Add --device if you want to specify GPU or CPU\n",
    "eval_command.extend(['--device', '0']) # Use GPU 0\n",
    "# eval_command.extend(['--device', 'cpu']) # Use CPU\n",
    "\n",
    "try:\n",
    "    # Execute the evaluation command\n",
    "    process = subprocess.Popen(eval_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    stdout, stderr = process.communicate()\n",
    "\n",
    "    print(\"STDOUT:\")\n",
    "    print(stdout.decode('utf-8'))\n",
    "    print(\"STDERR:\")\n",
    "    print(stderr.decode('utf-8'))\n",
    "\n",
    "    if process.returncode != 0:\n",
    "        print(f\"Evaluation failed with return code {process.returncode}\")\n",
    "    else:\n",
    "        print(\"Evaluation completed successfully.\")\n",
    "        # The evaluation results (mAP, precision, recall, etc.) will be printed to stdout\n",
    "        # and saved in the 'runs/test/yolov7-tiny-test-eval' directory.\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during evaluation: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Evaluation completed.\")\n",
    "print(\"Check the output above and the 'runs/test' directory for results.\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Note: The path to the best weights (`best_weights_path`) is a placeholder.\n",
    "# You need to manually identify the best training run from your grid search results\n",
    "# (e.g., by checking the validation metrics printed during training or on Wandb)\n",
    "# and update `best_weights_path` to point to the `weights/best.pt` file of that specific run.\n",
    "# You might need to list the directories in `yolov7/runs/train` to find the correct path."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNjtrVHHWNQbVhv9gpHOgis",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "yolov7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
